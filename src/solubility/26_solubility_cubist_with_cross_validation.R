setwd("//media//kswada//MyFiles//R//solubility")

packages <- c("dplyr", "AppliedPredictiveModeling", "caret", "lattice", "MASS", "gbm")
purrr::walk(packages, library, character.only = TRUE, warn.conflicts = FALSE)



# ------------------------------------------------------------------------------
# data:  solubility
# ------------------------------------------------------------------------------
data("solubility", package = "AppliedPredictiveModeling")

dim(solTrainX)

names(solTrainX)

head(solTestY)

str(solTrainX)


fingerprints <- grep("FP", names(solTrainXtrans))
length(fingerprints)


# rpart only uses formulas, so we put the predictors and outcome into a common data frame first.
trainData <- solTrainXtrans
trainData$y <- solTrainY



# ------------------------------------------------------------------------------
# Create folds explicitly
# ------------------------------------------------------------------------------
set.seed(100)


# create folds explicitly, default is 10 folds
indx <- createFolds(solTrainY, returnTrain = TRUE)
indx



# ------------------------------------------------------------------------------
# Set train control
# ------------------------------------------------------------------------------
ctrl <- trainControl(method = "cv", index = indx)



# ------------------------------------------------------------------------------
# Regression Trees and Rule-Based Models:  Cubist
#   - Some specific differences between Cubist and approaches for model trees and their rule-based variants are:
#       - The specific techniques used for linear model smoothing, creating rules, and pruning are different
#       - An optional boosting -- like procedure called "committees"
#       - The predictions generated by the model rules can be adjusted using nearby points from the training set data
#
# ------------------------------------------------------------------------------

# ------------------------------------------------------------------------------
# Tune Cubist model by cross-validation
#   - An R package called Cubist was created using the open-source code.
#     The function does not have an formula method since it is desirable to have the Cubist code manage the criterion and usage of dummy variables
# ------------------------------------------------------------------------------
library(doMC)
registerDoMC(10)


# The function fits the ruled-based model described in Quinlan (1992) (aka M5) with additional corrections
# based on nearest neighbors in the training set, as described in Quinlan (1993a)

# committees:  how many committee models (e.g., boosting iterations) should be used ?
# neighbors:  a single integer value (between 0 and 9) to adjust the rule-based predictions from the training set.
#             The choice of instance-based corrections does not need to be made until samples are predicted

# cbGrid <- expand.grid(committees = c(1:10, 20, 50, 75, 100, 125), neighbors = c(0, 1, 5, 9))
cbGrid <- expand.grid(committees = c(1:10, 20, 50, 75, 100), neighbors = c(0, 1, 5, 9))

set.seed(100)

cubistTune <- train(x = solTrainXtrans, y = solTrainY, method = "cubist",
                    tuneGrid = cbGrid, trControl = ctrl)


cubistTune

cubistTune$finalModel



# ----------
# Independent of the number of neighbors used, there is a trend where the error is significantly reduced as the number of committees is increased
# and then stabilizes around 50 committees.
# The use of the training set to adjust the model predictions is interesting:
#  - a purely rule-based model does better than an adjustment with a single neighbor, but  the error is reduced the most when 9 neighbors are used.
# In the end, the model with the lowest error (0.57 log units) was associated with 100 committees and an adjust ment using 9 neighbors,
# although fewer committees could also be used without much loss of performance.
# For the final Cubist model, the average number of rules per committees was 5.1 but ranged from 1 to 15.
plot(cubistTune, auto.key = list(columns = 4, lines = TRUE))



# ----------
# There is no established technique for measuring predictor importance for Cubist modesl.
# One can enumerate how many times a predictor variable was used in either a linear model or a split and use these tabulations to get a rough idea
# the impact each predictor has on the model. However, this approach ignores the neighbor-based correction that is sometimes used by Cubist.
# The modeler can choose how to weight the counts for the splits and the linear models in the overall usage calculation.

# This shows the predictor importance values calculated for the model with 100 committees and correct the prediction using the 9-nearest
# neighbors.  x-axis is the total usage of the predictor (i.e., the number of times it was used in a split or a linear model).

# The continuous predictors appear to have a greater impact on the model than the fingerprint descriptors.
cbImp <- varImp(cubistTune, scale = FALSE)

cbImp



# ------------------------------------------------------------------------------
# diagnose models
# ------------------------------------------------------------------------------
testRes_cubist <- data.frame(obs = solTestY, cubist = predict(cubistTune, solTestXtrans))
testRes_cubist <- testRes_cubist %>% mutate(resid = obs - cubist)



# ----------
# diagnositc plot
axisRange <- extendrange(c(testRes_cubist$obs, testRes_cubist$cubist))

graphics.off()
par(mfrow = c(1,2))
with(testRes_cubist, plot(obs, cubist, ylim = axisRange, xlim = axisRange, pch="*", col="red"));  abline(0, 1, col = "darkgrey", lty = 2)
with(testRes_cubist, plot(cubist, resid, pch="*", col = "blue"));  abline(h = 0, col = "darkgrey", lty = 2)



# ------------------------------------------------------------------------------
# train Cubist model by Cubist
# ------------------------------------------------------------------------------
library(Cubist)

cubistMod0 <- cubist(x = solTrainXtrans, y = solTrainY)

cubistMod1 <- cubist(x = solTrainXtrans, y = solTrainY, committees = 100)

cubistMod0
cubistMod1



# ----------
# summary function generates the exact rules that were used, as well as the final smoothed linear model for each rule.
summary(cubistMod0)
summary(cubistMod1)



# ----------
# The choice of instance-based corrections does not need to be made until samples are predicted
predict(cubistMod1, solTestXtrans, neighbors = 9)


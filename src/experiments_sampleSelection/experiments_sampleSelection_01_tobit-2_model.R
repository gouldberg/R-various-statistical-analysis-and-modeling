setwd("//media//kswada//MyFiles//R//experiments_sampleSelection")

packages <- c("dplyr", "sampleSelection")
purrr::walk(packages, library, character.only = TRUE, warn.conflicts = FALSE)



# ------------------------------------------------------------------------------
# Create bivariate normal disturbances with correlation
# ------------------------------------------------------------------------------

library(mvtnorm)


# create bivariate normal disturbances with correlation = -0.7
eps <- rmvnorm(500, c(0, 0), matrix(c(1, -0.7, -0.7, 1), 2, 2))

eps


# ----------
# ordinaly least squares regression
eps_mod <- lm(eps[,2] ~ eps[,1])

summary(eps_mod)


par(mfrow = c(1, 1))
plot(eps[,2] ~ eps[,1], pch = 20)
abline(eps_mod, col = "blue", lwd = 2)



# ------------------------------------------------------------------------------
# variable for selection equation
# ------------------------------------------------------------------------------

# xs:  uniformly distributed explanatory variable for the selection equations
xs <- runif(500)


# ys:  the selection outcome
ys <- xs + eps[, 1] > 0



# ----------
# selection equation
par(mfrow = c(1, 1))

plot(ys ~ xs, pch = 20)



# ------------------------------------------------------------------------------
# variable for outcome equation, observable variable with exclusion restriction
# ------------------------------------------------------------------------------

# xo:  explanatory variable for the outcome equation
# Note that xo is independent from xs
# the exclusion restriction -- independent information about the selection process -- has a certain identifying power
xo <- runif(500)


# yoX:  latent outcome variable
yoX <- xo + eps[, 2]


# yo:  observable variable
yo <- yoX * (ys > 0)



# ------------------------------------------------------------------------------
# Model comparison
# ------------------------------------------------------------------------------

# true model
mod_true <- lm(yoX ~ xo)
summary(mod_true)


# model with only observable, but reported as zero for non-reported
mod_1 <- lm(yo ~ xo)
summary(mod_1)


# model with only observable, excluding non-reported
idx <- which(ys > 0)
mod_2 <- lm(yo[idx] ~ xo[idx])
summary(mod_2)



# ----------
# tobit-2 model with exlusing restriction

library(sampleSelection)
tobit2_mod <- selection(ys ~ xs, yo ~ xo)
summary(tobit2_mod)
coef(tobit2_mod)



# ----------
graphics.off()
par(mfrow = c(1, 1))

# observable and unobservable
plot(xo[idx], yo[idx], pch = 20, xlim = c(0.0, 1.0), ylim = c(-3, 4), xlab = "", ylab = "")
par(new = T)
plot(xo[-idx], yoX[-idx], pch = 1, xlim = c(0.0, 1.0), ylim = c(-3, 4), xlab = "", ylab = "")

# true model
abline(a = coef(tobit2_mod)[1], b = coef(tobit2_mod)[2], col = "blue", lty = 1, lwd = 2)
abline(a = coef(mod_true)[1], b = coef(mod_true)[2], col = "blue", lty = 2, lwd = 2)

# only observable model
abline(a = coef(mod_2)[1], b = coef(mod_2)[2], col = "black", lty = 3, lwd = 1)



# -->
# filled circle:  observable outcomes
# empty circle:  unobservable outcomes
# solid line:  true dependence
# dashed line:  ML estimate by sampleSelection
# dotted line:  OLS estimate based on observable outcomes only


# -->
# We can see that the unobserved values (empty circles) tend to have higher yo realizations than the observed ones (filled circles)
# This is because rho = -0.7 < 0
# The OLS estimate (dotted line) is substantially downward biased. -- it does not take into account the fact that we tend to observe the observations
# with low realizations



# ------------------------------------------------------------------------------
# Tobit-2 model without exclusion restriction
# ------------------------------------------------------------------------------

# without exclusion restriction
# yo2 is generated by xs (used in original selection equation) not by xo
# here, yo2 is NOT independent from selection process

yoX2 <- xs + eps[, 2]

yo2 <- yoX2 * (ys > 0)



# ----------
# true model
mod_true_2 <- lm(yoX2 ~ xs)
summary(mod_true_2)


# model with only observable, but reported as zero for non-reported
mod_1_2 <- lm(yo2 ~ xs)
summary(mod_1_2)


# model with only observable, excluding non-reported
idx <- which(ys > 0)
mod_2_2 <- lm(yo2[idx] ~ xs[idx])
summary(mod_2_2)



# ----------
# tobit-2 model without exlusing restriction
tobit2_mod2 <- selection(ys ~ xs, yo2 ~ xs)
summary(tobit2_mod2)
coef(tobit2_mod2)



# ----------
summary(tobit2_mod2)

summary(tobit2_mod)



# -->
# still unbiased but standard errors are substantially larger in this case.
# the exclusion restriction -- independent information about the selection process -- has a certain identifying power that we now have lost.


# ----------
graphics.off()
par(mfrow = c(1, 1))

# observable and unobservable
plot(xs[idx], yo2[idx], pch = 20, xlim = c(0.0, 1.0), ylim = c(-3, 4), xlab = "", ylab = "")
par(new = T)
plot(xs[-idx], yoX2[-idx], pch = 1, xlim = c(0.0, 1.0), ylim = c(-3, 4), xlab = "", ylab = "")

# true model
abline(a = coef(tobit2_mod2)[1], b = coef(tobit2_mod2)[2], col = "blue", lty = 1, lwd = 2)
abline(a = coef(mod_true_2)[1], b = coef(mod_true_2)[2], col = "blue", lty = 2, lwd = 2)

# only observable model
abline(a = coef(mod_2_2)[1], b = coef(mod_2_2)[2], col = "black", lty = 3, lwd = 1)




# ------------------------------------------------------------------------------
# Tobit-2 model:  illustrative example with more variation in xs
# ------------------------------------------------------------------------------

# change the support of xs from [0, 1] to [-5, 5]
xs3 <- runif(500, -5, 5)

# selection outcome
ys3 <- xs3 + eps[, 1] > 0

# latente outcome
yoX3 <- xs3 + eps[, 2]

# observable outcome
yo3 <- yoX3 * (ys3 > 0)



# ----------
# true model
mod_true_3 <- lm(yoX3 ~ xs3)
summary(mod_true_3)


# model with only observable, but reported as zero for non-reported
mod_1_3 <- lm(yo3 ~ xs3)
summary(mod_1_3)


# model with only observable, excluding non-reported
idx3 <- which(ys3 > 0)
mod_2_3 <- lm(yo3[idx3] ~ xs3[idx3])
summary(mod_2_3)



# ----------
tobit2_mod3 <- selection(ys3 ~ xs3, yo3 ~ xs3)

summary(tobit2_mod3)

summary(tobit2_mod2)

summary(tobit2_mod)


coef(tobit2_mod3)



# -->
# Note all the parameters are precisely estimated, with even higher precision than in the first example
# where the exclusion restriction is fulfilled. 
# Selection is not an issue if xs > 2 while virtually nothing is observed if xs < -2



# ----------
graphics.off()
par(mfrow = c(1, 1))

# observable and unobservable
plot(xs3[idx3], yoX3[idx3], pch = 20, xlim = c(-5, 5), ylim = c(-7, 7), xlab = "", ylab = "")
par(new = T)
plot(xs3[-idx3], yoX3[-idx3], pch = 1, xlim = c(-5, 5), ylim = c(-7, 7), xlab = "", ylab = "")

# true model
abline(a = coef(tobit2_mod3)[1], b = coef(tobit2_mod3)[2], col = "blue", lty = 1, lwd = 2)
abline(a = coef(mod_true_3)[1], b = coef(mod_true_3)[2], col = "blue", lty = 2, lwd = 2)

# only observable model
abline(a = coef(mod_2_3)[1], b = coef(mod_2_3)[2], col = "black", lty = 3, lwd = 1)


